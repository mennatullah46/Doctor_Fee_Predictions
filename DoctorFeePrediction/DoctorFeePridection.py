# -*- coding: utf-8 -*-
"""Copy of Final Ml project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKQkwtp57274j2Hy5de8ADo7SMOqbmhh

# **Doctor Fee Prediction**
![alt text](https://pearlhealth.com/wp-content/uploads/2022/12/Machine-Learning__Feature.png)

# Team :

* Nour Sameh El Barbary
* Hoor Hisham
* Nour El Mohammady
* Rawan Badr
* Mennatallah Maged

#instruction : comment the visualisation in cities for running faster
# pip install category_encoders
"""

# pip install category_encoders

"""# Importing libraries and Reading Data[link text](https://)"""

import pandas as pd
import numpy as np


import time
from sklearn.ensemble import GradientBoostingRegressor

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import missingno as msno
from tabulate import tabulate

# Check if the data is normally distributed
from scipy.stats import shapiro, boxcox
from sklearn.preprocessing import PowerTransformer
import statsmodels.api as sm

# Preprocessing
from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, MultiLabelBinarizer, LabelEncoder
import re
import category_encoders as ce

# For modeling
from scipy import stats
from scipy.stats import norm
from scipy.stats.mstats import winsorize
from geopy.geocoders import Nominatim

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint
from sklearn.ensemble import AdaBoostRegressor
import xgboost as xgb

from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline


import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import uniform, randint
from sklearn.model_selection import train_test_split
import plotly.graph_objects as go
import time
from sklearn.ensemble import GradientBoostingRegressor
from lightgbm import LGBMRegressor

df = pd.read_csv('DoctorFeePrediction.csv')

"""#Data Description

* **Doctor Name**: The name of the doctor.
* **City**: The city where the doctor is located.
* **Specialization**: The area of specialization or medical expertise of the doctor.
* **Doctor Qualification**: The qualifications or degrees held by the doctor.
* **Experience (Years)**: The number of years of experience the doctor has.
* **Total Reviews**: The total number of reviews received by the doctor.
* **Patient Satisfaction Rate (%age)**: The percentage of patients satisfied with the doctor's services.
* **Avg Time to Patients (mins)**: The average time taken by the doctor to attend to patients.
* **Wait Time (mins)**: The average wait time for patients before being attended to by the doctor.
* **Hospital Address**: The address of the hospital where the doctor practices.
* **Doctors Link**: A link to the doctor's profile or information.
* **Fee (PKR)**: The consultation fee charged by the doctor .

# **Task 1: Explore and Familiarize with the Dataset:**

![alt text](https://assets-global.website-files.com/62c609e220cfd73d2f4f179b/639146ee91bb9016fd8cf0a3_Data%20Exploration%20(1).png)
"""

df.shape

"""* Dataset consists of **```2386 rows```** and **```12 columns```**.

##1 Dive into the dataset to uncover any peculiarities or unexpected patterns that may influence linear regression modeling.



##2.Utilize domain knowledge to identify potential interactions or nonlinear relationships between features and the target variable **"Fee(PKR)"**.
"""

df.head().style.background_gradient('Blues')

df.info()

df.dtypes

"""**Dataset Overview :**
1. No null values but there may be missing values
2. data need to be encoded "Doctor Name" , "City", "Specialization" , "Doctor Qualification" ,"Hospital Address", "Doctors Link"
"""

df.duplicated().sum()

duplicate_rows = df[df.duplicated()]
duplicate_rows

#1
def drop_duplicates(df):
    df.drop_duplicates(inplace=True)

drop_duplicates(df)

df.shape

df.nunique()

df.isnull().sum()

"""Satistics for **categorical** columns."""

df.describe()

df.describe(include='O').T

"""found out that most freq in these 2 cols 'Hospital Address' & 'Doctors Link	' No Address Available & No Link Available	**So** we may drop them"""

def rename_columns(df):
    df.rename(columns={'Fee(PKR)': 'Fee',
                       'Patient Satisfaction Rate(%age)': 'Patient_Satisfaction_Rate',
                       'Experience(Years)': 'Experience_Years',
                       'Avg Time to Patients(mins)': 'Avg_time_per_Patient',
                       'Wait Time(mins)': 'Wait_Time'}, inplace=True)

rename_columns(df)

rename_columns_transformer = FunctionTransformer(rename_columns)


pipeline = Pipeline([
    ('rename_columns', rename_columns_transformer)
])

categorical_columns = df.select_dtypes(include='object').columns
categorical_columns

"""# 1.Doctor Name

## Cleaning
"""

df[['Doctor Name']].describe().T

doctor_name_counts = df['Doctor Name'].value_counts()
duplicate_doctor_names = doctor_name_counts[doctor_name_counts > 1]
print(duplicate_doctor_names)

# Find repeated doctor names

duplicate_doctor_names = doctor_name_counts[doctor_name_counts > 1].index

print(duplicate_doctor_names)

duplicate_data = df[df['Doctor Name'].isin(duplicate_doctor_names)]

sorted_duplicate_data = duplicate_data.sort_values(by='Doctor Name')

sorted_duplicate_data.head()

for name in sorted_duplicate_data['Doctor Name'].unique():
    doctor_data = sorted_duplicate_data[sorted_duplicate_data['Doctor Name'] == name]

    print('----------------------------------------')
    print(f"Doctor Name: {name}")
    print('----------------------------------------')

    doctor_data

    print(f"Number of rows: {len(doctor_data)}")

"""the duplicated names maybe Similarity of names we will see that when se more relation with other columns like 'Link'

"""

#1
def extract_titles_and_clean_name(name):
    # Define a regex pattern for titles
    title_regex = r'(Dr\.|Prof\.|Mr\.|Ms\.|Colonel|Assoc\. Prof\. Dr\.|Asst\. Prof\. Dr\.|Prof\. Dr\.)'

    # List of accepted titles
    accepted_titles = ["Dr", "Asst Prof Dr", "Prof, Dr", "Assoc Prof Dr"]

    # Find all titles in the name
    titles = re.findall(title_regex, name)

    # Clean the name by removing the extracted titles
    cleaned_name = re.sub(title_regex, '', name).strip()

    # Convert titles to a cleaned string without periods
    title_str = ', '.join(titles).replace('.', '').strip()

    # Check if the concatenated title string is in the list of accepted titles
    if title_str not in accepted_titles:
        title_str = 'others'

    return title_str, cleaned_name


df[['Titles', 'Cleaned Name']] = df['Doctor Name'].apply(lambda x: pd.Series(extract_titles_and_clean_name(x)))

df['Doctor Name'] = df['Cleaned Name']
df.drop('Cleaned Name', axis=1, inplace=True)

df.head()

df['Titles'].nunique()

empty_title_rows = df[df['Titles'] == '']

# Display these rows
empty_title_rows

title_counts = df['Titles'].value_counts()
title_counts

duplicate_entries = df[df.duplicated(subset=['Doctor Name', 'Specialization','City'], keep=False)]
sorted_dup = duplicate_entries.sort_values(by=['Doctor Name', 'Specialization', 'City','Fee'])

sorted_dup

"""will drop row of dr that have  same ['Doctor Name', 'Specialization','City'] and also 'Doctors Link' as it is wierd to have dr have same all things"""

def drop_duplicate_rows(df):
    df = df.drop_duplicates(subset=['Doctor Name', 'Specialization', 'City', 'Doctors Link'], keep=False)
    return df

# df.info()

"""## Analysis"""

doctor_count_by_city = df['City'].value_counts().reset_index()
doctor_count_by_city.columns = ['City', 'Number of Doctors']

# Create the bar plot using Plotly Express
fig = px.bar(doctor_count_by_city, x='City', y='Number of Doctors',
             title='Number of Doctors in Each City',
             labels={'Number of Doctors': 'Number of Doctors', 'City': 'City'})
fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability
fig.show()

"""In conclusion, the analysis reveals that Karachi has the highest number of doctors compared to other cities in the dataset. However, it's worth noting that several cities have only one doctor listed. This discrepancy in the distribution of doctors across cities might indicate variations in healthcare accessibility and resource allocation."""

city_counts = df['City'].value_counts()

cities_with_1_doctor = city_counts[city_counts == 1].index
cities_with_144_doctors = city_counts[city_counts == 144].index

# Now, filter the DataFrame for doctors in these cities
doctors_in_cities_with_1 = df[df['City'].isin(cities_with_1_doctor)]
doctors_in_cities_with_144 = df[df['City'].isin(cities_with_144_doctors)]

# Print the fees of doctors in these cities
print("Fees of doctors in cities with 1 doctor:")
print(doctors_in_cities_with_1['Fee'])

print("\nFees of doctors in cities with 144 doctors:")
print(doctors_in_cities_with_144['Fee'])

fees_1_doctor = df[df['City'].isin(cities_with_1_doctor)]['Fee']

# Fees of doctors in cities with 144 doctors
fees_144_doctors = df[df['City'].isin(cities_with_144_doctors)]['Fee']


# Create a box plot
plt.figure(figsize=(8, 6))
plt.boxplot([fees_1_doctor, fees_144_doctors], labels=['1 Doctor', '144 Doctors'])
plt.title('Comparison of Doctor Fees in Cities with 1 Doctor vs. 144 Doctors')
plt.xlabel('City Count')
plt.ylabel('Doctor Fee')
plt.show()

# Calculate summary statistics for fees of cities with 1 doctor
summary_1_doctor = fees_1_doctor.describe()

# Calculate summary statistics for fees of cities with 144 doctors
summary_144_doctors = fees_144_doctors.describe()

print("Summary Statistics for Fees in Cities with 1 Doctor:")
print(summary_1_doctor)

print("\nSummary Statistics for Fees in Cities with 144 Doctors:")
print(summary_144_doctors)

"""Cities with **1** doctor have a fee range from **0 to 1000**, whereas cities with **144** doctors have a wider fee range from **$200 to 5000** This indicates greater variability in the fees charged by doctors in cities with a higher concentration of doctors.



* avg comparing

In cities with only 1 doctor, the mean fee charged is significantly lower at approximately 493.33 compared to cities with 144 doctors, where the mean fee is substantially higher at approximately 1524.31. This suggests that there is a noticeable difference in the average fees depending on the number of doctors in a city.

#2. City

## Cleaning
"""

df[['City']].describe().T

df['City'].unique()

cities = df['City'].unique()

# ## comment this vis for faster running

# import folium

# # Creating a geolocator object
# geolocator = Nominatim(user_agent="city_locator", timeout=45)  # Timeout set to 10 seconds


# #  centroid of all cities
# total_lat, total_lon = 0, 0
# num_cities = len(cities)
# for city in cities:
#     location = geolocator.geocode(city)
#     if location:
#         total_lat += location.latitude
#         total_lon += location.longitude

# centroid_lat = total_lat / num_cities
# centroid_lon = total_lon / num_cities

# # Create a map centered around the centroid
# m = folium.Map(location=[centroid_lat, centroid_lon], zoom_start=5, control_scale=True, width='60%', height='60%')

# # Add markers for each city
# for city in cities:
#     location = geolocator.geocode(city)
#     if location:
#         folium.Marker(location=[location.latitude, location.longitude], popup=city, icon=folium.Icon(color='red')).add_to(m)

# # Save the map to an HTML file
# m.save('cities_map.html')

# # Display the map
# m

"""## Feature Engineering (Region)"""

#3
def assign_region_clean_city(df):
    # Define the city-region mappings
    punjab_cities = [
        'Lahore', 'Islamabad', 'Multan', 'Sahiwal', 'Okara', 'Faisalabad', 'Sargodha',
        'Gujranwala', 'Rawalakot', 'Gujrat', 'Sialkot', 'Sheikhupura', 'Kasur', 'Narowal',
        'Jhang', 'Khanewal', 'Toba Tek Singh', 'Chiniot', 'Pakpattan', 'Burewala', 'Vehari',
        'Rahim Yar Khan', 'Bahawalpur', 'Bahawalnagar', 'Lodhran', 'Layyah', 'Mianwali',
        'Muzaffar Garh', 'Dera Ghazi Khan', 'Bhakkar', 'Khushab', 'Mian Channu', 'Chichawatni',
        'Gojra', 'Shorkot', 'Samundri', 'Tando Muhammad Khan', 'Talagang', 'Kamoke', 'Shahkot',
        'Dinga', 'Bhalwal', 'Chakwal', 'Kharian', 'Daska', 'Hafizabad', 'Sadiqabad', 'Nankana Sahib',
        'Pattoki', 'Alipur', "Rajan Pur", "Jhelum", "Attock", "Lalamusa", "Wah Cantt",
        "Dunyapur", "Khanpur", "Kot Addu", "Mandi Bahauddin", "Renala Khurd",
        "Taxila", "Jauharabad", "Gujar Khan", "Wazirabad", "Pasrur",
        "Muridke", "Chishtian", "Kabirwala", "Jaranwala", "Dijkot"
    ]

    sindh_cities = [
        'Karachi', 'Hyderabad', 'Mirpur Khas', 'Sukkur', 'Nawabshah', 'Larkana', 'Jacobabad',
        'Khairpur', 'Thatta', 'Jamshoro', 'Ghotki', 'Shikarpur', 'Badin', 'Dadu', 'Khairpur Nathan Shah',
        'Moro', 'Hala', "Kandiaro", "Umarkot", "Kashmor", "Mithi", "Matiari", "Shahdadpur", "Baden"
    ]

    kpk_cities = [
        'Peshawar', 'Abbottabad', 'Nowshera', 'Swabi', 'Mardan', 'Mansehra', 'Haripur', 'Bannu',
        'Kohat', 'Dera Ismail Khan', 'Mingora', 'Charsadda', 'Timergara', 'Buner', 'Chitral', 'Dargai',
        "Hangu", "Swat", "Malakand", "Bajaur Agency"
    ]

    balochistan_cities = [
        'Quetta', 'Turbat', 'Chaman', 'Khuzdar', 'Gwadar', 'Loralai', 'Zhob', 'Sibi', 'Nushki',
        'Barkhan', 'Mastung', 'Duki'
    ]

    international_cities = [
        "Istanbul", "Riyadh", "Izmir"
    ]

    kashmir_cities = [
        'Gilgit', 'Kotli', 'Mirpur', 'Skardu'
    ]

    # Create a dictionary to map cities to their respective regions
    city_regions = {
        **{city: 'Punjab Region' for city in punjab_cities},
        **{city: 'Sindh Region' for city in sindh_cities},
        **{city: 'KPK Region' for city in kpk_cities},
        **{city: 'Balochistan Region' for city in balochistan_cities},
        **{city: 'International Region' for city in international_cities},
        **{city: 'Kashmir Region' for city in kashmir_cities}
    }

    # Clean the 'City' column (el 7eta elly htro7 el test script)
    df['City'] = df['City'].str.replace('-', ' ').str.title()

    df['Region'] = df['City'].map(city_regions)

    return df

df = assign_region_clean_city(df)

df["City"].nunique()

# df['City'].value_counts()

"""## Analysis"""

FeesbyCity= df[['City','Fee']].groupby('City').agg('mean').sort_values('Fee')

fig = px.bar(FeesbyCity.reset_index(), x='City', y='Fee', color='City', text='Fee')

fig.update_traces(marker=dict(line=dict(color='#000000', width=1.2)))

fig.update_layout(title='<b>Average Doctor Fee by City</b>', title_x=0.5, font_family="Times New Roman", title_font_family="Times New Roman")

fig.show()

"""the avg fees are in LAHORE city lets dive into it and see fees of drs in this city , whilr in IZMIR is the lowes avg tends to 0"""

lahore_city_fees = df[df['City'] == 'Lahore']['Fee']
lahore_min_fee = lahore_city_fees.min()
lahore_max_fee = lahore_city_fees.max()

print("Minimum fee in LAHORE:", lahore_min_fee)
print("Maximum fee in LAHORE:", lahore_max_fee)

# Calculate average doctor fee by city
fees_by_city = df[['City', 'Fee']].groupby('City').agg('mean').sort_values('Fee', ascending=False)

top_5_cities = fees_by_city.head(5).reset_index()

fig = px.bar(top_5_cities, x='City', y='Fee', color='City', text='Fee')

fig.update_traces(marker=dict(line=dict(color='#000000', width=1.2)))

fig.update_layout(title='<b>Average Doctor Fee by City (Top 5)</b>', title_x=0.5, font_family="Times New Roman", title_font_family="Times New Roman")

fig.show()

"""The fees vary considerable, with a standard deviation of approximately 1369.37, suggesting a wide range of fee amounts."""

average_fees_by_city = df.groupby('City')['Fee'].mean()

city_with_min_avg_fees = average_fees_by_city.idxmin()
min_avg_fees = average_fees_by_city.min()

print(f"The city with the minimum average fees is {city_with_min_avg_fees} with an average fee of ${min_avg_fees:.2f}.")

izmir_reviews = df[df['City'] == 'Izmir']

total_reviews_in_izmir = izmir_reviews['Total_Reviews'].sum()

print(f"The total number of reviews in Izmir is {total_reviews_in_izmir}.")

izmir_rows = df[df['City'] == 'Izmir']

izmir_rows

df.shape

# Computes the maximum fee within each city group
MaxFeesByCity = df.groupby('City')['Fee'].max().reset_index()

fig = px.bar(MaxFeesByCity, x='City', y='Fee', color='City', text='Fee')

fig.update_traces(marker=dict(line=dict(color='#000000', width=1.2)))

fig.update_layout(title='<b>Maximum Doctor Fee by City</b>',
                  title_x=0.5,
                  font_family="Times New Roman",
                  title_font_family="Times New Roman")

fig.show()

gujranwala_rows = df[df['City'] == 'Gujranwala']

gujranwala_fee_stats = gujranwala_rows['Fee'].describe()

print("Summary Statistics for Fees in GUJRANWALA:")
print(gujranwala_fee_stats)

"""a wide range of fees charged by doctors in the city, with a minimum fee of 20 and a maximum fee of 10,000. The mean fee of approximately 1259.53 indicates the average cost of medical services

----------------------
(IQR) of 800 (from 700 to 1500) suggests that the middle 50% of fees are relatively consistent, with the median fee (1000) falling within this range. This indicates that while there is considerable variability in fees, **a significant portion of doctors charge fees within a certain range**
"""

df['Region'].value_counts()

df['Region'].isnull().sum()

"""
data=df.groupby(['Fee','Specialization']).apply(lambda x:x['Region'].count()).reset_index(name='Region')
px.line(data,x='Fee',y='Region',color='Specialization',title='')
"""

df_above_7k = df[df['Fee'] > 7000]

# Get unique hospital addresses for each region
addresses_by_region = df_above_7k.groupby('Region')['Hospital Address'].unique()


# Print addresses for each region
for region, addresses in addresses_by_region.items():
    print(f"Addresses with fees above 7k in {region}:")
    for address in addresses:
        print(address)

"""*  the largest fee is 7500 for Nephrologist in  Punjab Region
* the smallest   the  fee is 0 for Nephrologist in  Punjab Region also

"""

# x=df.groupby('City').agg({'Specialization':'count'}).sort_values(by='Specialization',ascending=False).reset_index()
# x

"""#3.Specialization

## Cleaning
"""

df[['Specialization']].describe().T

specialization_counts = df['Specialization'].value_counts()

# Print the counts
print(specialization_counts)

#2
def all_specialization_preprocessing(df):
  specialization_mapping = {
    "Pediatrician,Pediatric": "Pediatrician",
    "Lung Specialist": "Pulmonologist",
    "Eye Surgeon,Eye Specialist": "Ophthalmologist",
    "Sexologist": "Andrologist",
    "Cosmetic Surgeon,Dermatologist": "Cosmetic Dermatologist",
    "Internal Medicine Specialist,General Physician,Infectious Diseases": "Infectious Disease Specialist",
    'Dermatologist, Dermatologist, Allergy Specialist': 'Dermatologist, Allergy Specialist',
    'Plastic Surgeon, Cosmetic Surgeon, Plastic Surgeon, Dermatologist': "Cosmetic Dermatologist",


  }
  #clean
  def process_specialization(entry):
    entry = entry.replace('/', ',')

    specialties = [s.strip() for s in entry.split(',')]
    unique_specialties = []
    for specialty in specialties:
        if specialty not in unique_specialties:
            unique_specialties.append(specialty)
    # Join back into a string
    unique_specialties_str = ','.join(unique_specialties)
    return unique_specialties_str
  df['Specialization'] = df['Specialization'].apply(process_specialization)
  #map
  def map_specialization(specialization):
      for key, value in specialization_mapping.items():
          if key in specialization:
              return value
      return specialization
  df['Specialization'] = df['Specialization'].apply(map_specialization)

all_specialization_preprocessing(df)

df['Specialization'].nunique()

top_15_specializations = df['Specialization'].value_counts().head(15)

print("Top 15 most appeared specializations:")
print(top_15_specializations)

df.info()

"""## Analysis"""

#doctors for each specialization
specialization_counts = df['Specialization'].value_counts()

popular_specializations = specialization_counts[specialization_counts >= 183].index

popular_specialization_rows = df[df['Specialization'].isin(popular_specializations)]

fees_of_popular_specializations = popular_specialization_rows['Fee']

data = {'Specialization': popular_specialization_rows['Specialization'], 'Fee': fees_of_popular_specializations}
df_plot = pd.DataFrame(data)

fig = px.bar(df_plot, x='Specialization', y='Fee', title='Fees of Specializations with 183 Doctors or More')
fig.show()

max_fees = popular_specialization_rows.groupby('Specialization')['Fee'].max()
min_fees = popular_specialization_rows.groupby('Specialization')['Fee'].min()

for specialization in popular_specializations:
    print(f"Specialization: {specialization}")
    print(f"Maximum Fee: {max_fees[specialization]}")
    print(f"Minimum Fee: {min_fees[specialization]}")
    print()

df.Specialization.value_counts()

plt.figure(figsize=(12, 6))

top_10_specializations = df['Specialization'].value_counts().head(10).index

filtered_df = df[df['Specialization'].isin(top_10_specializations)]

sns.barplot(x='Specialization', y='Fee', data=filtered_df)

# Rotate the x-axis labels by 45 degrees
plt.xticks(rotation=45)

# Show the plot
plt.show()

data = df.groupby(['Experience_Years', 'Specialization']).apply(lambda x: x['Patient_Satisfaction_Rate'].count()).reset_index(name='Rating')

fig = px.line(data, x='Experience_Years', y='Rating', color='Specialization', title='Avg of Ratings by Experience Years and Specialization')
fig.show()

"""Drs with less experiene have highest rating and dr that are 47 yrs are not given a rate"""

top_20_specializations = df.groupby('Specialization')['Fee'].mean().nlargest(20).index

# Filter the DataFrame to include only data for the top 20 specializations
df_top_20_specializations = df[df['Specialization'].isin(top_20_specializations)]

# Calculate maximum and minimum fees by specialization for the top 20 specializations
max_fees = df_top_20_specializations.groupby('Specialization')['Fee'].max()
min_fees = df_top_20_specializations.groupby('Specialization')['Fee'].min()

# Create a DataFrame for plotting
plot_data = pd.DataFrame({'Specialization': max_fees.index,
                          'Max Fee': max_fees.values,
                          'Min Fee': min_fees.values})

# Plot the data
fig = px.bar(plot_data, x='Specialization', y=['Max Fee', 'Min Fee'],
             barmode='group', title='Maximum and Minimum Fees by Specialization for Top 20 Specializations',
             labels={'value': 'Fee', 'Specialization': 'Specialization'})
fig.show()

"""Nephrologist,Diabetologist apears to have same max and min fees so lets view it

"""

filtered_rows = df[df['Specialization'].str.contains('Nephrologist') & df['Specialization'].str.contains('Diabetologist')]

filtered_rows.head()

"""## Feature Engineering (Specialization Count)"""

specialization_counts = df['Specialization'].str.count(',') + 1

doctors_with_multiple_specializations = specialization_counts[specialization_counts >= 1]

fig = px.histogram(doctors_with_multiple_specializations, x='Specialization', title='Distribution of Doctors with One or More Specializations')
fig.update_xaxes(title='Number of Specializations')
fig.update_yaxes(title='Count')
fig.show()

multi_specialization_rows = df[df['Specialization'].str.count(',') >= 1]

multi_specialization_rows

num_multi_specialization_rows = multi_specialization_rows.shape[0]

print("Number of Dr with multiple specializations:", num_multi_specialization_rows)

"""lets see if ppl with 1 specialization are getting more fees or not"""

single_specialization = df[df['Specialization'].apply(lambda x: len(x.split(','))) == 1]

fig = px.scatter(single_specialization, x='Specialization', y='Fee', title='Fees for Doctors with One Specialization',
                 labels={'Specialization': 'Specialization', 'Fee': 'Fee'})

fig.update_layout(xaxis_tickangle=-45)

fig.show()

df['Specialization Count'] = df['Specialization'].str.count(',') + 1

max_specialization_count = df['Specialization Count'].max()
min_specialization_count = df['Specialization Count'].min()

print("Maximum number of specializations:", max_specialization_count)
print("Minimum number of specializations:", min_specialization_count)

"""drs that have 1 or 2 specialization are the most ppl that take higer fees that is expected 3shan e7na bnb2a 3wziin nroo7 le dactra mot5sesiin msh btoo3 kol 7aga

## Encoding

"
def clean_text(text):
    if pd.isna(text):
        return []
    parts = text.split(',')
    cleaned_parts = []
    for part in parts:
        part = re.sub(r'\([^)]*\)', '', part)
        part = re.sub(r'[.]', '', part)
        part = re.sub(r'\bAND\b', '', part, flags=re.IGNORECASE)
        cleaned_part = part.upper().strip()
        cleaned_parts.append(cleaned_part)

    filtered_list = [value for value in cleaned_parts if not value.startswith("CERTIFIED") and not value.startswith("CERTIFICATION")
                     and not value.startswith("CERTIFICATE")]
    return filtered_list

# Assuming df['Specialization'] exists
df['Specialization'] = df['Specialization'].apply(clean_text)

encoded_dfs = {}
cols = ['Specialization']
for col in cols:
    binarizer = MultiLabelBinarizer()
    # Make sure to pass list of lists to the binarizer
    encoded_df = pd.DataFrame(binarizer.fit_transform(df[col]),
                              columns=binarizer.classes_,
                              index=df.index)
    encoded_dfs[col] = encoded_df

# Display the encoded DataFrame
encoded_dfs['Specialization']

df_encoded = pd.concat([df, encoded_df], axis=1)
df_encoded
"""

# df = pd.concat([df, encoded_dfs['Specialization']], axis=1)

#df_encoded.columns

for col in df.columns:
  print('-'*40)
  print(f"{col} nulls = {df[col].isnull().sum()}")

"""## Encoding top 15 momkn 10 brdo shofo"""

# # Assuming df is your DataFrame

# # Instantiate the TargetEncoder
# encoder = ce.TargetEncoder(cols=['Specialization'])

# # Fit and transform the encoder on the 'Specialization' column using 'Fee' as the target
# df['Specialization_encoded'] = encoder.fit_transform(df['Specialization'], df['Fee'])

# # Now, df['Specialization_encoded'] contains the target-encoded values for 'Specialization'

df.info()

# #11
# onehot_encoded = pd.get_dummies(df['Specialization']).astype(int)

# df_encoded = pd.concat([df, onehot_encoded], axis=1)
# df_encoded

# df = df_encoded

"""#4. Doctor Qualification"""

df[['Doctor Qualification']].describe().T

"""## **NLP**

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

# Download NLTK resources (if not already downloaded)
#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')

# Initialize Lemmatizer and CountVectorizer
lemmatizer = WordNetLemmatizer()
vectorizer = CountVectorizer()

# Tokenization and Lemmatization
def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # Tokenization and convert to lowercase
    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization
    return ' '.join(tokens)

# Remove stopwords and perform lemmatization
stop_words = set(stopwords.words('english'))

# Apply preprocessing to each value in the 'Doctor Qualification' column
df['Cleaned Qualifications'] = df['Doctor Qualification'].apply(preprocess_text)

# Vectorization using Bag of Words
X = vectorizer.fit_transform(df['Cleaned Qualifications'])

# X now contains the vectorized representation of 'Doctor Qualification' column
"""

df['Doctor Qualification'].explode().value_counts()[:60]

"""there are typo error so will remove it

## Cleaning

---
"""

df[['Doctor Qualification']].describe().T

df["Doctor Qualification"].nunique()

df["Doctor Qualification"].unique()

# df['Doctor Qualification'].explode().unique().shape

df['Doctor Qualification'].explode().value_counts()[:50]

#12
def clean_qualifications(df):
    # Combine and update all replacements into a single dictionary
    replacements = {
        r'\bPhD\b': 'PHD', r'\bM\.D\.\b': 'MD', r'\bD\.M\.S\b': 'DMS',
        r'\bB\.Sc\.\b': 'BSC', r'\bM\.S\.\b': 'MS', r'\bM\.Phil\b': 'MPHIL',
        r'\bG\.A\.M\.S\b': 'GAMS', r'\(D\.H\.B\)': 'DHB', r'\(D\.Ac\)': 'PHD',
        r'Ophtamology': 'Ophthalmology', r'Gastroentrology': 'Gastroenterology',
        r'OtoRhinoLaryngology': 'Otorhinolaryngology', r'Paediatrics': 'Pediatrics',
        r'Pulmonology': 'Pulmonary', r'ENT': 'Otolaryngology', r'OrthopedicSurgery': 'Orthopedic Surgery',
        r'NeuroSurgery': 'Neurosurgery', r'Medicine': 'Internal Medicine',
        r'OBSTETRICS&GYNAECOLOGY': 'Obstetrics&Gynecology', r'Gynecology&amp;Obstetrics': 'Gynecology and Obstetrics',
        r'Genecology&amp;Obstetrics': 'Gynecology and Obstetrics', r'OtorhinolaryngologicENT': 'Otorhinolaryngologic,ENT',
        r'MasterOfSurgery': 'Master of Surgery', r'MD\d*': 'MD', r'MDGastroenterology': 'MD,Gastroenterology',
        r'FCPSPediatrics': 'FCPS,Pediatrics', r'MBBSMD': 'MBBS,MD', r'FRCSOrthopedics': 'FRCS,Orthopedics',
        r'MCPSGynae/Obs': 'MCPS(Gynecology/Obs)', r'MD-RMP': 'MD, RMP', r'Masters\(NeuroSurgeon\)': 'Masters, Neurosurgery',
        r'\(|\)': '', r'[^a-zA-Z,]': '', r'Ophthalmologist': 'Ophthalmology', r'GASTROENTEROLOGY': 'Gastroenterology',
        r'MCPS,': 'MCPS', r'M\.D': 'MD', 'MD 1': 'MD'
    }

    # Apply all replacements
    df['Doctor Qualification'] = df['Doctor Qualification'].replace(replacements, regex=True)

    # Additional replacements to handle specific concatenations
    concatenations = {
        r'FCPSOBSTETRICSampGYNAECOLOGY': 'FCPS,Obstetrics&Gynecology',
        r'FCPSOtolaryngology': 'FCPS,Otolaryngology',
        r'MCPSFCPS': 'MCPS,FCPS'
    }
    df['Doctor Qualification'] = df['Doctor Qualification'].replace(concatenations, regex=True)

    # Remove all unnecessary spaces, then remove spaces around commas
    df['Doctor Qualification'] = df['Doctor Qualification'].str.replace(r'\s+', '')
    df['Doctor Qualification'] = df['Doctor Qualification'].str.replace(r'\s*,\s*', ',', regex=True)
    df['Doctor Qualification'] = df['Doctor Qualification'].str.replace(r'\([^)]*\)', '', regex=True)


    # Enhanced cleaning function
    def enhance_cleaning(qualification):
        # Replace HTML entities and correct specific cases
        qualification = qualification.replace('&amp;', '&')
        qualification = re.sub(r'(?<!\w)([A-Z]+)(?!\w)', lambda x: x.group(1), qualification)
        qualification = qualification.replace('DiplomainTBandChestDiseases', 'DTBCD')

        # Split, sort, and remove duplicates
        parts = sorted(set(qualification.split(',')))  # Remove duplicates and sort
        return ','.join(parts)

    # Apply the enhanced cleaning function
    df['Doctor Qualification'] = df['Doctor Qualification'].apply(enhance_cleaning)

    return df

df = clean_qualifications(df)

"""## Analysis"""

# dr qualification that appears only once in the data
qualification_counts = df['Doctor Qualification'].value_counts()


qualifications_count_1 = qualification_counts[qualification_counts == 1]


print(qualifications_count_1)

print(df['Doctor Qualification'].unique())

#drs that dont have mbbs

rows_without_MBBS = df[~df['Doctor Qualification'].str.contains('MBBS')]

rows_without_MBBS


#conclusion : 89 rows

non_MBBS_rows = df[~df['Doctor Qualification'].str.contains('MBBS')]

# el drs el m3ndhomsh mbbs
print("Qualifications of rows where 'MBBS' is not present:")
for index, row in non_MBBS_rows.iterrows():
    print(row['Doctor Qualification'])

"""* 96.2% of the drs have MBBs Qualification
* 3.8 % only dont have MBBs.

most of drs have MBBS (Bachelor of Medicine, Bachelor of Surgery)
"""

# el shahada di w5dha kam dr

qualified_doctors_counts = qualification_counts[qualification_counts > 14]



fig = px.bar(x=qualified_doctors_counts.index, y=qualified_doctors_counts.values,
             labels={'x': 'Doctor Qualification', 'y': 'Count'},
             title='Doctor Qualifications with More Than 14 Doctors')
fig.show()

qualification_counts = df['Doctor Qualification'].str.count(',') + 1

# Find the maximum and minimum number of qualifications
max_qualifications = qualification_counts.max()
min_qualifications = qualification_counts.min()

print(f"Maximum number of qualifications: {max_qualifications}")
print(f"Minimum number of qualifications: {min_qualifications}")

"""## Feature Engineering (Number of Qualification)"""

df['No_of_qualifications'] = df['Doctor Qualification'].apply(lambda x: len(x.split(",")))
df['No_of_qualifications'].value_counts()

sns.countplot(x='No_of_qualifications',data=df)
plt.xticks(rotation=45)
plt.show()

df[df['No_of_qualifications'] == 10]

df[df['No_of_qualifications'] == 9]

import plotly.express as px

# Create a copy of the DataFrame
modified_df = df.copy()

# Find all values beyond label 5
values_to_replace = modified_df['No_of_qualifications'].value_counts()[4:].index

# Replace those values with 'Others'
modified_df.loc[modified_df['No_of_qualifications'].isin(values_to_replace), 'No_of_qualifications'] = 'Others'

# Plot the pie chart
fig = px.pie(modified_df['No_of_qualifications'].value_counts(),
             values=modified_df['No_of_qualifications'].value_counts().values,
             names=modified_df['No_of_qualifications'].value_counts().index,
             title='Distribution of Number of Qualifications',
             labels={'index': 'Number of Qualifications'})  # Changing the label here

fig.show()

"""most doctors have 2 qualifications
 1 has 10 and onther have 11 so will see them and put it to the best qualifications
"""

fig = px.box(df, x='No_of_qualifications', y='Fee', title='Boxen Plot of Fee by Number of Qualifications')

fig.show()

"""As the number of qualifications of doctors increase, So does there fees increases, but after 8 qualifications fees seem to decrease and then increase at 12

#5.Hospital Address

## Analysis
"""

df['Hospital Label'] = np.where(df['Hospital Address'] == 'No Address Available', 0, 1)

# Calculate average fees by hospital label
avg_fees_by_label = df.groupby('Hospital Label')['Fee'].mean().reset_index()

# Creating scatter plot with trendline
fig = px.scatter(df, x='Hospital Label', y='Fee', trendline='ols', title='Hospital Address VS avg Fees')

# Add average fees line to the plot
fig.add_scatter(x=avg_fees_by_label['Hospital Label'], y=avg_fees_by_label['Fee'], mode='lines', line=dict(dash='dot', color='red'))

fig.show()

df.drop('Hospital Label', axis=1, inplace=True)

"""* Doctors in these hospitals have the largest fees

"""

df_above_7k = df[df['Fee'] > 7000]

# Get unique hospital addresses for each region
addresses_by_region = df_above_7k.groupby('Region')['Hospital Address'].unique()

# Print addresses for each region
for region, addresses in addresses_by_region.items():
    print(f"Addresses with fees above 7k in {region}:")
    for address in addresses:
        print(address)

df['Hospital Address'].value_counts()

filtered_doctors = df[(df['Hospital Address'] == 'No Address Available') & (df['Doctors Link'] == 'No Link Available')]
print(filtered_doctors[['Doctor Name']])

df["Doctors Link"].replace("No Link Available", pd.NA, inplace=True)
df["Hospital Address"].replace("No Address Available", pd.NA, inplace=True)

"""##feature engnieering has hospital address"""

#15
df['has_Doctors_Link'] = df['Doctors Link'].notnull().astype(int)

df['has_hospital_address'] = df['Hospital Address'].notnull().astype(int)

df["Hospital Address"].isnull().sum()

msno.matrix(df)

#16
df["Doctors Link"].replace(pd.NA, "No Link Available", inplace=True)

df["Hospital Address"].replace(pd.NA, "No Address Available", inplace=True)

df.info()

hospital_address_counts = df['Hospital Address'].value_counts()

print(hospital_address_counts.head(20))

hospital_address_counts = df['Hospital Address'].value_counts()

avg_fees_by_address = df.groupby('Hospital Address')['Fee'].mean()

address_data = pd.DataFrame({'Count': hospital_address_counts, 'Average Fees': avg_fees_by_address})

address_data_sorted = address_data.sort_values(by='Count', ascending=False).head(20)

plt.figure(figsize=(12, 6))
address_data_sorted['Count'].plot(kind='bar', color='skyblue', alpha=0.7, label='Count')
plt.ylabel('Count')
plt.xlabel('Hospital Address')
plt.xticks(rotation=45, ha='right')
plt.twinx()
address_data_sorted['Average Fees'].plot(kind='line', color='orange', label='Average Fees')
plt.ylabel('Average Fees')
plt.title('Counts of Hospital Addresses with Average Fees')
plt.legend()
plt.tight_layout()
plt.show()

"""Doctors Hospital, Johar Town,Lahore  hase the highest avg fees , it appeared 6 times in df                                      """

# Filter the DataFrame to include only the rows with the hospital address "Doctors Hospital, Johar Town, Lahore"
hospital_address = "Doctors Hospital, Johar Town, Lahore"
filtered_df = df[df['Hospital Address'] == hospital_address]

# Check if there are any rows for the specified hospital address
if not filtered_df.empty:
    # Find the doctor(s) with the highest fees
    highest_fees = filtered_df['Fee']

    # Display the fees of doctors with the highest fees
    doctors_with_highest_fees = filtered_df[filtered_df['Fee'] == highest_fees]
    print("Doctors with the highest fees at", hospital_address, ":\n")
    print(doctors_with_highest_fees[['Doctor Name', 'Fee']])
else:
    print("No data found for the hospital address:", hospital_address)

# Filter the DataFrame to include only the rows with the hospital address "Doctors Hospital, Johar Town, Lahore"
hospital_address = "Doctors Hospital, Johar Town, Lahore"
filtered_df = df[df['Hospital Address'] == hospital_address]

# Check if there are any rows for the specified hospital address
if not filtered_df.empty:
    # Find the highest fee among doctors at this hospital
    highest_fee = filtered_df['Fee']

    # Display the doctors with the highest fee
    doctors_with_highest_fees = filtered_df[filtered_df['Fee'] == highest_fee]
    print("\nDoctors with the highest fee at", hospital_address, ":\n")
    print(doctors_with_highest_fees.to_string(index=False))
else:
    print("\nNo data found for the hospital address:", hospital_address)

"""### No Address Available"""

filtered_df = df[df['Hospital Address'] == 'No Address Available']

print(filtered_df[['Hospital Address', 'City']])

"""#6.Doctors Link"""

df[['Doctors Link']].describe().T

df.head()

df['Doctors Link'].value_counts()

filtered_df = df[df['Doctors Link'] != 'No Link Available']

# Identify duplicate values in the filtered DataFrame
duplicate_doctors_link = filtered_df[filtered_df.duplicated(subset=['Doctors Link'], keep=False)]

# Print the DataFrame with duplicate links
print(duplicate_doctors_link[['Doctors Link']])

################
filtered_df = df[df['Doctors Link'] != 'No Link Available']

duplicate_doctors_link = filtered_df[filtered_df.duplicated(subset=['Doctors Link'], keep=False)]

sorted_duplicate_doctors = duplicate_doctors_link.sort_values(by='Doctor Name')

# View the sorted DataFrame
sorted_duplicate_doctors[['Doctor Name', 'Doctors Link' , 'Hospital Address', 'Doctor Qualification','Specialization']]

#### if same 'Doctor Name', 'Doctors Link', 'Hospital Address drop

# Drop duplicate rows based on 'Doctor Name', 'Doctors Link', and 'Hospital Address'
df.drop_duplicates(subset=['Doctor Name', 'Doctors Link' , 'Hospital Address'], inplace=True)

# Verify the changes
df

value_counts = df['Doctors Link'].value_counts()

# Filter the values to include only those that are greater than 1 and not equal to 'No Link Available'
filtered_values = value_counts[(value_counts > 1) & (value_counts.index != 'No Link Available')]

# Get all data corresponding to the filtered values
filtered_data = df[df['Doctors Link'].isin(filtered_values.index)]
# Sort the filtered data by a specific column, for example, 'Doctor Name'
sorted_filtered_data = filtered_data.sort_values(by='Doctor Name')
# Print the sorted data
sorted_filtered_data

"""* mmkn ndrop"""

value_counts = df['Doctors Link'].value_counts()
filtered_values = value_counts[value_counts > 1]
print(filtered_values)

value_counts = df['Doctors Link'].value_counts()

filtered_values = value_counts[(value_counts > 1) & (value_counts.index != 'No Link Available')]

filtered_data = df[df['Doctors Link'].isin(filtered_values.index)]


sorted_filtered_data = filtered_data.sort_values(by='Doctor Name')

sorted_filtered_data.head()

print("Count of filtered data:", sorted_filtered_data.shape[0])

df.shape

no_link_data = df[df['Doctors Link'] == 'No Link Available']
no_link_data.head()

df['Doctors Link'] = df['Doctors Link'].replace('No Link Available', 'https://instacare.pk/')

"""# **Numerical Values**

Satistics for **Numerical** columns.
"""

numerical_columns = df.select_dtypes(include=['int','float']).columns
numerical_columns

numerical_columns=['Experience_Years', 'Total_Reviews', 'Patient_Satisfaction_Rate', 'Avg_time_per_Patient', 'Wait_Time', 'Fee','No_of_qualifications']

df.describe().T

df.hist(figsize=(15, 15))

"""we concluded that
1. experience is somehow normally distributed
2. total_reviews is right skewed and the max is > 2000 & min is almost 0
3.
4.try log to cancel skewness df['total_rooms'] = np.log(df['total_rooms'] +1)

"""

plt.figure(figsize=(20,30))
for i, col in enumerate(df[numerical_columns].columns):
    ax = plt.subplot(9, 2, i+1)
    sns.distplot(df[col], ax=ax,kde=False)
    plt.xlabel(col)

plt.show()

# Filter numerical columns
numerical_cols = df.select_dtypes(include='number').columns

# Plot KDE for numerical columns
for col in numerical_cols:
    plt.figure(figsize=(5, 5))
    sns.kdeplot(data=df, x=col, color='blue')
    plt.title(f'KDE Plot of {col}')
    plt.show()
    print("\n")

for column in numerical_columns:
    plt.figure(figsize=(10, 4))
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot for {column}')
    plt.grid(True)
    plt.show()

"""#7.Experience(Years)"""

### plot experierence > 10 with specialization w el fees

df[['Experience_Years']].describe().T

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Experience_Years'],color = 'turquoise')
plt.subplot(1,2,2)
sns.histplot(x = df['Experience_Years'], kde=True ,color = 'turquoise')

print('Minimum',df['Experience_Years'].min())
print('Maximum',df['Experience_Years'].max())

df.Experience_Years.value_counts()

"""concluded that there are drs that have experience yrs 1.5 and 4.5 so will round them"""

#17
df['Experience_Years'] = df['Experience_Years'].round()

df.Experience_Years.value_counts()

"""most drs have experience 10 yrs"""

fig = px.histogram(df, x='Experience_Years', color='Experience_Years',
                   title='Distribution of Experience Years',
                   labels={'Experience_Years': 'Experience Years', 'count': 'Frequency'},
                   color_discrete_sequence=px.colors.qualitative.Pastel)

fig.update_layout(xaxis=dict(type='category', categoryorder='array', categoryarray=df['Experience_Years'].value_counts().index))

fig.show()

df.Experience_Years.value_counts()

sns.scatterplot(x="Experience_Years", y="Fee", data=df)
print(df['Fee'].corr(df['Experience_Years']))
# Experience has almost no relationship with the fees

"""exprience varies from range of 1-53 & most are 10 yrs and right skewed"""

df_sorted = df.sort_values(by='Fee')

fig = px.bar(df_sorted, x='Experience_Years', y='Fee',
             title='Relationship between Experience and Fee',
             labels={'Experience_Years': 'Experience Years', 'Fee': 'Fee'},
             color='Experience_Years',
             color_discrete_sequence=px.colors.qualitative.Pastel)

fig.update_layout(xaxis=dict(categoryorder='total ascending'))

fig.show()

max_fee_by_experience = df.groupby('Experience_Years')['Fee'].max()
min_fee_by_experience = df.groupby('Experience_Years')['Fee'].min()

# Create a DataFrame for visualization
plot_data = pd.DataFrame({'Experience_Years': max_fee_by_experience.index,
                          'Max_Fee': max_fee_by_experience.values,
                          'Min_Fee': min_fee_by_experience.values})

# Plot the data
fig = px.line(plot_data, x='Experience_Years', y=['Max_Fee', 'Min_Fee'],
              title='Maximum and Minimum Fees vs. Experience Years',
              labels={'value': 'Fee', 'Experience_Years': 'Experience Years'})
fig.show()

"""* AT max_fees : wee got that drs with experience 5 or more yrs are maximum then there is a drop at 24yrs then gets high again
* min_fees varies as exp varies to sum us fees is not that related to experience as there is a wide range in fees as it depends more on market demand , specialization ,Patient Demographics
"""

sns.lmplot(x='Experience_Years',y='Fee',data=df,height=6, aspect=1.2)

bins = [0, 5, 10, 15, 20, 25, 30]
# labels = ['0-5', '6-10', '11-15', '16-20', '21-25', '26-30']
labels = ['Novice', 'Beginner', 'Competent', 'Proficient', 'Expert', 'Master']


df['Experience_Group'] = pd.cut(df['Experience_Years'], bins=bins, labels=labels, right=False)

group_counts = df['Experience_Group'].value_counts().sort_index()

# Plotting
plt.bar(group_counts.index, group_counts.values, color='skyblue')
plt.xlabel('Experience Group')
plt.ylabel('Count')
plt.title('Distribution of Experience Groups')
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.show()

"""* mmkn ndrop"""

fig = px.scatter(df, x='Experience_Group', y='Fee', title='Fee by Experience Group')
fig.show()

fig = px.bar(df, x='Experience_Group', y='Fee', title='Average Fees by Experience Group')

# Show the plot
fig.show()

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Experience_Group'],color = 'red')
plt.subplot(1,2,2)
sns.histplot(x = df['Experience_Group'], kde=True ,color = 'red')

print('Minimum',df['Experience_Group'].min())
print('Maximum',df['Experience_Group'].max())

df.dtypes

"""#8.Total_Reviews"""

#### visualisation b kol el columns yo3tbar

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Total_Reviews'],color = 'turquoise')
plt.subplot(1,2,2)
sns.histplot(x = df['Total_Reviews'], kde=True ,color = 'turquoise')

print('Minimum',df['Total_Reviews'].min())
print('Maximum',df['Total_Reviews'].max())

df['Total_Reviews'].value_counts()

"""most of reviews are 0"""

df['Review_or_Not'] = (df['Total_Reviews'] > 0).astype(int)

df.info()

# Get unique values of the column
unique_patient_satisfaction_rates = df['Total_Reviews'].unique()

# Print the unique values
print("Unique values in the column:")
print(unique_patient_satisfaction_rates)

# Print count of each unique value


# Print the number of unique values
num_unique_values = len(unique_patient_satisfaction_rates)
print("\nNumber of unique values:", num_unique_values)

fig = px.box(df, x='Region', y='Total_Reviews', color='Region',
             title='Box Plot: Total Reviews by Region')
fig.show()

"""least reviews at international regions and kashmir region"""

fig = px.bar(df, x='Region', y='Review_or_Not', title='Bar Plot: Total Reviews by Region (Unique Values)')
fig.show()

df_filtered = df[df['Total_Reviews'] == 1]

# Create a bar plot using Plotly Express
fig = px.bar(df_filtered, x='Specialization', title='Specializations with Total Reviews Equal to 1')
fig.show()

fig = px.scatter(df, x='Patient_Satisfaction_Rate', y='Total_Reviews',
                 title='Scatter Plot: Patient Satisfaction Rate vs Total Reviews')
fig.show()

plt.figure(figsize=(10, 6))
plt.scatter(df['Total_Reviews'], df['Fee'], color='skyblue', alpha=0.6)
plt.title('Total Reviews vs Fees')
plt.xlabel('Total Reviews')
plt.ylabel('Fees')
plt.grid(True)
plt.show()

#total reviews doesnt correlate so much with fee

"""#9.Patient Satisfaction Rate(%age)"""

# Get unique values of the column
unique_patient_satisfaction_rates = df['Patient_Satisfaction_Rate'].unique()

# Print the unique values
print("Unique values in the column:")
print(unique_patient_satisfaction_rates)

# Print count of each unique value

# Print the number of unique values
num_unique_values = len(unique_patient_satisfaction_rates)
print("\nNumber of unique values:", num_unique_values)

"""from 33 -100"""

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Patient_Satisfaction_Rate'],color = 'red')
plt.subplot(1,2,2)
sns.histplot(x = df['Patient_Satisfaction_Rate'], kde=True ,color = 'red')

print('Minimum',df['Patient_Satisfaction_Rate'].min())
print('Maximum',df['Patient_Satisfaction_Rate'].max())

satisfaction_rate_counts = df['Patient_Satisfaction_Rate'].value_counts()

satisfaction_rate_counts_sorted = satisfaction_rate_counts.sort_index(ascending=True)

print("\nCount of each unique value (Patient_Satisfaction_Rate sorted in ascending order):")
print(satisfaction_rate_counts_sorted)

"""we can categorize ratings to high , med , low"""

df['Patient_Satisfaction_Rate'].describe()

"""min rate is 33 and max is 100"""

df['Patient_Satisfaction_Rate'].value_counts(normalize=True)[:25].plot(kind='barh')

"""most have rating 100%
 minimum rating as 33%.
3. highly skewed to the left and there are outliers
"""

df_copy = df.copy()

# Divide 'Patient_Satisfaction_Rate' values by 1000
df_copy['Patient_Satisfaction_Rate'] /= 1000

# Create a bar plot using Plotly Express
fig = px.bar(df_copy, x='Wait_Time', y='Patient_Satisfaction_Rate', title='Wait Time vs. Patient Satisfaction Rate (in Thousands)')

# Update y-axis tick labels to display values in thousands
fig.update_yaxes(tickformat='.0f', title='Patient Satisfaction Rate (Thousands)')

# Show the plot
fig.show()

"""if the waiting time for the dr but the dr is good so it is worth the wait so it doesnt affect

if fees is min which is minimum and equal to =0 satesfaction rate seems high

#10.Avg Time to Patients(min)
"""

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Avg_time_per_Patient'],color='turquoise')
plt.subplot(1,2,2)
sns.histplot(x = df['Avg_time_per_Patient'], kde=True , color='turquoise')

print('Minimum',df['Avg_time_per_Patient'].min())
print('Maximum',df['Avg_time_per_Patient'].max())

"""A pie chart that shows that the majority of the patients spend of average less than 15 mins"""

unique_avg_time_per_patient = df['Avg_time_per_Patient'].unique()
print(unique_avg_time_per_patient)

plt.figure(figsize=(10, 6))
plt.scatter(df['Avg_time_per_Patient'], df['Fee'], color='blue', alpha=0.5)
plt.axhline(y=df['Fee'].mean(), color='red', linestyle='--', label='Average Fee')
plt.title('Relation between Avg Time to Patients and Fee')
plt.xlabel('Avg Time per Patient')
plt.ylabel('Fee')
plt.legend()
plt.grid(True)
plt.show()

"""#11.Wait Time(mins)

"""

df['Wait_Time']

df['Wait_Time'].isnull().sum()

max_value = df['Wait_Time'].max()
min_value = df['Wait_Time'].min()
print('max val = ' , max_value)
print('min val = ' , min_value)

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Wait_Time'],color='turquoise')
plt.subplot(1,2,2)
sns.histplot(x = df['Wait_Time'], kde=True , color='turquoise')

# # Set the style of the plot
# sns.set(style="whitegrid")

# # Create a bigger figure
# plt.figure(figsize=(28, 8))

# # Create a bar plot using Seaborn
# sns.barplot(x='Wait_Time', y='Fee', data=df, palette="viridis")  # Change palette to "viridis"
# plt.xlabel('Wait Time')
# plt.ylabel('Fee')
# plt.title('Fee vs Wait Time')

# # Show the plot
# plt.show()


# #group by average fees per each waittime

avg_fee_per_wait_time = df.groupby('Wait_Time')['Fee'].mean().reset_index()

# Set the style of the plot
sns.set(style="whitegrid")

# Create a bigger figure
plt.figure(figsize=(28, 8))

# Create a bar plot using Seaborn
sns.barplot(x='Wait_Time', y='Fee', data=avg_fee_per_wait_time, palette="viridis")  # Change palette to "viridis"
plt.xlabel('Wait Time')
plt.ylabel('Average Fee')
plt.title('Average Fee vs Wait Time')

# Show the plot
plt.show()

"""## Feature Engineering (Total Time)"""

#18
df['Total Time'] = df['Avg_time_per_Patient'] + df['Wait_Time']

df

"""#12.Fee(PKR)"""

df['Fee'].value_counts()

"""There is imbalance in the target so will use stratify
To prevent bias and ensures that our model learns from a balanced distribution of target classes


"""

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(x = df['Fee'],color='turquoise')
plt.subplot(1,2,2)
sns.histplot(x = df['Fee'], kde=True , color='turquoise')

print('Minimum',df['Fee'].min())
print('Maximum',df['Fee'].max())

sns.histplot(df['Fee'], kde=True)

# Annotate the point indicating noise
plt.annotate('Noise', xy=(10000, 10), xytext=(12000, 400), color='blue', fontsize=9,
             arrowprops=dict(facecolor='blue', shrink=0.01))

plt.show()

""" some dr dont take fees but others take 10000 majority takes 1500"""

# df2 = df
# df1 = df



"""#Processing"""

df.info()

numerical_columns = df.select_dtypes(exclude=['object', 'category'])

# Calculate correlation matrix
corr_matrix = numerical_columns.corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Heatmap of Numerical Columns')
plt.show()

"""will drop reviews or not ass it gives same corr as total reviews

"""

df.drop(['Review_or_Not'], axis=1, inplace=True)
df.drop(['Avg_time_per_Patient'], axis=1, inplace=True)
df.drop(['Wait_Time'], axis=1, inplace=True)

# Filter columns with dtype 'object' or 'category'
categorical_columns = df.columns[df.dtypes.isin(['object', 'category'])].tolist()

cardinality_ratios = {}

for col in categorical_columns:
    try:
        cardinality_ratio = len(df[col].unique()) / len(df)
        cardinality_ratios[col] = cardinality_ratio
    except TypeError:
        print(f"Skipping '{col}' column due to unsupported data type.")

for col, ratio in cardinality_ratios.items():

    print(f"Cardinality ratio for '{col}' column:", ratio)

"""Surly we will drop Doctor Name as CR is almost 1 & Hospital Address"""

#sure
df.drop(['Doctors Link'], axis=1, inplace=True)
df.drop(['Hospital Address'], axis=1, inplace=True)
df.drop(['Doctor Name'], axis=1, inplace=True)
df.drop(['Region'], axis=1, inplace=True)
df.drop(['No_of_qualifications'], axis=1, inplace=True)
df.drop(['Experience_Group'], axis=1, inplace=True)

from scipy.stats import pearsonr, spearmanr

numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Calculate Pearson correlation coefficients
pearson_corr = {}
for col in numerical_cols:
    pearson_corr[col] = pearsonr(df[col], df['Fee'])[0]

# Calculate Spearman correlation coefficients
spearman_corr = {}
for col in numerical_cols:
    spearman_corr[col] = spearmanr(df[col], df['Fee'])[0]

# Create DataFrames for Pearson and Spearman correlations
pearson_df = pd.DataFrame({'Feature': list(pearson_corr.keys()), 'Pearson Correlation': list(pearson_corr.values())})
spearman_df = pd.DataFrame({'Feature': list(spearman_corr.keys()), 'Spearman Correlation': list(spearman_corr.values())})

# Display tables
print("Pearson Correlation Table:")
print(pearson_df)

print("\nSpearman Correlation Table:")
print(spearman_df)

"""Experience_Years has a Pearson correlation coefficient of approximately 0.431, indicating a moderate positive linear relationship with Fee.
Total_Reviews has a Pearson correlation coefficient of approximately 0.306, indicating a moderate positive linear relationship with Fee.
Patient_Satisfaction_Rate has a Pearson correlation coefficient of approximately 0.030, indicating a weak positive linear relationship with Fee.
Total Time has a Pearson correlation coefficient of approximately 0.121, indicating a very weak positive linear relationship with Fee.
"""

##### henaaaa

df.info()

categorical_columns = df.select_dtypes(include='object').columns
categorical_columns

"""## Scaling Data"""

numerical_columns = df.select_dtypes(include='number').columns
numerical_columns

num_cols = ['Experience_Years','Total_Reviews','Total Time']

for col in num_cols:
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 2, 1)
    sns.boxplot(x=df[col], color='turquoise')
    plt.title(f'Boxplot of {col}')

    plt.subplot(1, 2, 2)
    sns.histplot(x=df[col], kde=True, color='turquoise')
    plt.title(f'Histogram with KDE of {col}')

    plt.show()

    print(f'Minimum of {col}: {df[col].min()}')
    print(f'Maximum of {col}: {df[col].max()}')

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def show_distributions(data, col):
    plt.figure(figsize=(12, 4))

    # Plotting the histogram without any transformation
    plt.subplot(1, 4, 1)
    sns.histplot(data[col], kde=True, color='purple')
    plt.title('Original Data')

    # Plotting the histogram with log transformation
    plt.subplot(1, 4, 2)
    sns.histplot(np.log1p(data[col]), kde=True, color='blue')
    plt.title('Log Transformation')

    # Plotting the histogram with cubic root transformation
    plt.subplot(1, 4, 3)
    sns.histplot(np.cbrt(data[col]), kde=True, color='red')
    plt.title('Cubic Root Transformation')

    # Plotting the histogram with square root transformation
    plt.subplot(1, 4, 4)
    sns.histplot(np.sqrt(data[col]), kde=True, color='green')
    plt.title('Square Root Transformation')

    plt.suptitle(f'Distributions for Column {col}', fontsize=16, y=1.05)

    plt.tight_layout()
    plt.show()

    # Display skewness for each transformation
    print(f"Skewness without transformation = {data[col].skew()}")
    print(f"Skewness with log transformation = {np.log1p(data[col]).skew()}")
    print(f"Skewness with cubic root transformation = {np.cbrt(data[col]).skew()}")
    print(f"Skewness with square root transformation = {np.sqrt(data[col]).skew()}")

sns.histplot(df['Fee'], kde=True)

# Annotate the point indicating noise
plt.annotate('Noise', xy=(10000, 10), xytext=(12000, 400), color='blue', fontsize=9,
             arrowprops=dict(facecolor='blue', shrink=0.01))

plt.show()

for col in num_cols:
    show_distributions(df,col)

def show_qq(data, col):
    plt.figure(figsize=(12, 4))

    # Plotting the Q-Q plot without any transformation
    plt.subplot(1, 4, 1)
    stats.probplot(data[col], dist="norm", plot=plt)
    plt.title('Original Data')
    plt.xlabel('Theoretical Quantiles')
    plt.ylabel('Sample Quantiles')

    # Plotting the Q-Q plot with log transformation
    plt.subplot(1, 4, 2)
    stats.probplot(np.log1p(data[col]), dist="norm", plot=plt)
    plt.title('Log Transformation')
    plt.xlabel('Theoretical Quantiles')
    plt.ylabel('Sample Quantiles')

    # Plotting the Q-Q plot with cubic root transformation
    plt.subplot(1, 4, 3)
    stats.probplot(np.cbrt(data[col]), dist="norm", plot=plt)
    plt.title('Cubic Root Transformation')
    plt.xlabel('Theoretical Quantiles')
    plt.ylabel('Sample Quantiles')

    # Plotting the Q-Q plot with square root transformation
    plt.subplot(1, 4, 4)
    stats.probplot(np.sqrt(data[col]), dist="norm", plot=plt)
    plt.title('Square Root Transformation')
    plt.xlabel('Theoretical Quantiles')
    plt.ylabel('Sample Quantiles')

    plt.suptitle(f'Q-Q Plot for Column {col}', fontsize=16, y=1.05)

    plt.tight_layout()
    plt.show()

"""Generating Q-Q plots for each transformation applied to visualize the degree of conformity to normal distribution and assess the effectiveness of the transformation in reducing skewness."""

for col in num_cols:
    show_qq(df,col)

def calculate_outliers(col):
    sorted(col)
    Q1, Q3 = col.quantile([0.25, 0.75])
    IQR = Q3 - Q1
    lower_range = Q1 - (1.5 * IQR)
    upper_range = Q3 + (1.5 * IQR)
    return lower_range, upper_range

for col in num_cols:
    lower_limit, upper_limit = calculate_outliers(df[col])
    lower_outliers = len(df[df[col] < lower_limit])
    upper_outliers = len(df[df[col] > upper_limit])
    total_outliers = lower_outliers + upper_outliers
    outlier_percentage = (total_outliers / df.shape[0]) * 100

    print(f"Total outliers in column before transformation {col}: {total_outliers}, Percentage: {outlier_percentage}%")
    print('-'*100)



# # Apply log transformation to 'Experience_Years' and 'Total_Reviews'
# df['Experience_Years'] = np.log1p(df['Experience_Years'])
# df['Total_Reviews'] = np.log1p(df['Total_Reviews'])

# # Apply cubic root transformation to 'Total_Time'
# df['Total Time'] = np.cbrt(df['Total Time'])

# df['Total_Reviews'] = df[''].apply(lambda x: x**(1/3))

for col in num_cols:
    lower_limit, upper_limit = calculate_outliers(df[col])
    lower_outliers = len(df[df[col] < lower_limit])
    upper_outliers = len(df[df[col] > upper_limit])
    total_outliers = lower_outliers + upper_outliers
    outlier_percentage = (total_outliers / df.shape[0]) * 100

    print(f"Total outliers in column aftar log transformation {col}: {total_outliers}, Percentage: {outlier_percentage}%")
    print('-'*100)

fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10))

# Flatten axes for easier iteration
axes = axes.flatten()

# Iterate over columns and create box plots
for i, col in enumerate(num_cols):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title("")  # Clear the title for individual plots

# Set a common title for all plots
fig.suptitle('Box Plots of Numeric Columns', fontsize=16)

# Adjust layout
plt.tight_layout()
plt.show()

#100
# numerical_cols = ['Experience_Years', 'Total_Reviews','Total Time']

# scaler = StandardScaler()

# # Iterate over each numerical column
# for col in numerical_cols:
#     # Fit and transform the column
#     df[col] = scaler.fit_transform(df[[col]])

numerical_cols = ['Experience_Years', 'Total_Reviews','Total Time']

for col in numerical_cols:

    plt.figure(figsize=(8, 4))
    sns.displot(x=df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

"""
## Scaling"""

# Set the style of the plot
sns.set(style="whitegrid")

# Create a KDE plot of the normalized 'Fee' column
plt.figure(figsize=(8, 6))
sns.kdeplot(df['Fee'], shade=True, color="skyblue")
plt.title('Kernel Density Estimation of Fee')
plt.xlabel('Normalized Fee')
plt.ylabel('Density')
plt.show()

"""## Encoding data"""

categorical_columns = df.select_dtypes(include='object').columns
categorical_columns

"""* Title"""

average_fee_by_title = df.groupby('Titles')['Fee'].mean()

# Plot the bar graph
plt.bar(average_fee_by_title.index, average_fee_by_title.values, color='skyblue')
plt.title('Average Fee by Title')
plt.xlabel('Title')
plt.ylabel('Average Fee')
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

print(df['Titles'].value_counts())
print('-'*100)
def encode_fee_by_title_avg_reverse(df, fee_col, title_col):
    # Calculate the average fee for each title
    average_fee_by_title = df.groupby(title_col)[fee_col].mean()

    # Rank the titles based on average fees in reverse order
    ranked_titles = average_fee_by_title.rank(method='dense', ascending=True).astype(int)

    # Subtract 1 from each rank to make it zero-based
    ranked_titles -= 1

    # Add encoding as a new column in the DataFrame
    encoded_df = df.copy()  # Create a copy to avoid modifying the original DataFrame
    encoded_df['Titles'] = ranked_titles.loc[df[title_col]].values

    return encoded_df


# Apply the function to encode the fee column
encoded_df = encode_fee_by_title_avg_reverse(df, 'Fee', 'Titles')





print( encoded_df['Titles'].value_counts())
print('-'*100)

def calculate_fee_group(df):
    # Grouping by 'Titles' and calculating the mean of 'Fee' for each group
    fee_by_title = df.groupby('Titles')['Fee'].mean()
    # Sorting the result by average fee in descending order
    fee_by_title_sorted = fee_by_title.sort_values(ascending=False)
    # Creating an encoding based on the sorted order
    encoding = {title: i for i, title in enumerate(fee_by_title_sorted.index)}
    # Encoding the titles based on the sorted order
    encoded_titles = df['Titles'].map(encoding)
    return encoded_titles

fee_by_title = calculate_fee_group(df)
print(fee_by_title)
df['Titles']= fee_by_title

df['Titles'].value_counts()

"""* Specialization encoding"""

top_15_specializations = df['Specialization'].value_counts().head(15)

df['Specialization'] = df['Specialization'].apply(lambda x: x if x in top_15_specializations.index else 'Others')

specialization_counts = df['Specialization'].value_counts()

print("Specialization Counts including 'Others':")
print(specialization_counts)
onehot_encoded = pd.get_dummies(df['Specialization']).astype(int)

df_encoded = pd.concat([df, onehot_encoded], axis=1)
df_encoded
import category_encoders as ce
df = df_encoded

specialization_mean_fee = df.groupby('Specialization')['Fee'].mean().sort_values()

# Plotting the mean fee for each specialization
plt.figure(figsize=(10, 6))
specialization_mean_fee.plot(kind='bar', color='skyblue')
plt.title('Mean Fee by Specialization')
plt.xlabel('Specialization')
plt.ylabel('Mean Fee')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

df.drop(['Specialization'], axis=1, inplace=True)

df.info()

import category_encoders as ce

target_encoder_qual = ce.TargetEncoder(cols=['Doctor Qualification'])
df['Doctor_Qualification_encoded'] = target_encoder_qual.fit_transform(df['Doctor Qualification'], df['Fee'])

# Target encoding for 'City'
target_encoder_city = ce.TargetEncoder(cols=['City'])
df['City_encoded'] = target_encoder_city.fit_transform(df['City'], df['Fee'])

# Drop original columns if needed
df.drop(['Doctor Qualification', 'City'], axis=1, inplace=True)

# Display the updated DataFrame
print(df.head())

print(df.head())



df.isna().sum()

# # Ensure 'Hospital Address' column contains string values
# df['Hospital Address'] = df['Hospital Address'].astype(str)

# # Combine Hospital Address into a single string without commas
# df['Hospital Address'] = df['Hospital Address'].apply(lambda x: ' '.join([item.strip() for item in x.split(', ')]))

# label_encoder = LabelEncoder()

# df['Hospital Address'] = label_encoder.fit_transform(df['Hospital Address'])

# df['Hospital Address'].unique()

# label_encoder = LabelEncoder()

# for col in to_encode:
#     df[col] = label_encoder.fit_transform(df[col])
#     print(df[col].unique())
#     print('-'*40)

# filePath=r'truefinaldf.csv'

#df.to_csv(filePath, index=False)

# numerical_cols = [col for col in df.columns if col != 'Fee']

# scaler = MinMaxScaler()

# # Iterate over each numerical column
# for col in numerical_cols:
#     # Fit and transform the column
#     df[col] = scaler.fit_transform(df[[col]])

# from sklearn.preprocessing import StandardScaler

# numerical_cols = [col for col in df.columns if col != 'Fee']

# scaler = StandardScaler()

# # Iterate over each numerical column
# for col in numerical_cols:
#     # Fit and transform the column
#     df[col] = scaler.fit_transform(df[[col]])

# # Display the scaled DataFrame
# print(df.head())

"""#Modeling and feature selection"""

num_columns = df.shape[1]

print("Number of columns in the DataFrame:", num_columns)

df = df.apply(pd.to_numeric, errors='coerce')

# Now, calculate the correlation
correlation_with_fee = df.corr()['Fee'].sort_values(ascending=False)

print("Correlation with 'fee' column:")
print(correlation_with_fee)

df.drop(['Specialization Count'], axis=1, inplace=True)

# correlation_with_fees = df.corr()['Fee'].sort_values(ascending=False)

# correlation_df = pd.DataFrame({'Columns': correlation_with_fees.index, 'Correlation': correlation_with_fees.values})

# fig = px.bar(correlation_df, x='Columns', y='Correlation', color='Correlation',
#              color_continuous_scale='blues', text='Correlation')

# fig.update_layout(title='Correlation with Fees Column',
#                   xaxis_title='Columns',
#                   yaxis_title='Correlation Coefficient',
#                   xaxis_tickangle=-90,
#                   width=3000, height=800)  # Set the width and height in pixels

# fig.show()

"""exp yrs ,ttl rev , n of qual"""

df.info()

df.shape

import time
from sklearn.ensemble import GradientBoostingRegressor

def evaluate_regression_models(X_train, X_test, y_train, y_test):

    np.random.seed(42)

    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(random_state=42),
        'Lasso Regression': Lasso(random_state=42),
        'Random Forest Regressor': RandomForestRegressor(random_state=42),
        'AdaBoost Regressor': AdaBoostRegressor(random_state=42),
        'XGBoost Regressor': xgb.XGBRegressor(random_state=42),
        # 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),
        # 'LGBM Regressor': LGBMRegressor(random_state=42,verbose=-1)  # Remove LGBMRegressor
    }

    best_hyperparameters = {}
    train_results = {'Model': [], 'Train MSE': [], 'Train RMSE': [], 'Train R2': [], 'Train Time (s)': []}
    test_results = {'Model': [], 'Test MSE': [], 'Test RMSE': [], 'Test R2': [], 'Test Time (s)': []}

    for model_name, model in models.items():
        param_distributions = {}
        if model_name in ['Ridge Regression', 'Lasso Regression']:
            param_distributions = {'alpha': uniform(0, 100)}
        elif model_name == 'Random Forest Regressor':
            param_distributions = {'n_estimators': randint(100, 200), 'max_depth': [None, 10, 20], 'min_samples_split': randint(2, 10)}
        elif model_name == 'AdaBoost Regressor':
            param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1)}
        elif model_name == 'XGBoost Regressor':
            param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1), 'max_depth': randint(2, 7)}


        # elif model_name in ['Gradient Boosting Regressor', 'LGBM Regressor']:  # Modify for LGBMRegressor
        #     param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1), 'max_depth': randint(2, 7)}

        start_train_time = time.time()
        random_search = RandomizedSearchCV(model, param_distributions, n_iter=10, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, random_state=42)
        random_search.fit(X_train, y_train)
        end_train_time = time.time()

        best_hyperparameters[model_name] = random_search.best_params_

        best_model = random_search.best_estimator_
        best_model.fit(X_train, y_train)

        # Print selected features
        print(f"Selected features for {model_name}: {X_train.columns}")

        start_pred_time_test = time.time()
        y_train_pred = best_model.predict(X_train)
        y_test_pred = best_model.predict(X_test)
        end_pred_time_test = time.time()

        train_mse, test_mse = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)
        train_rmse, test_rmse = np.sqrt(train_mse), np.sqrt(test_mse)
        train_r2, test_r2 = r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)

        train_time = (end_train_time - start_train_time)   # Convert to milliseconds
        test_time = (end_pred_time_test - start_pred_time_test)   # Convert to milliseconds

        train_results['Model'].append(model_name)
        train_results['Train MSE'].append(round(train_mse, 6))
        train_results['Train RMSE'].append(round(train_rmse, 6))
        train_results['Train R2'].append(round(train_r2, 6))
        train_results['Train Time (s)'].append(round(train_time / 1000, 6))  # Convert back to seconds

        test_results['Model'].append(model_name)
        test_results['Test MSE'].append(round(test_mse, 6))
        test_results['Test RMSE'].append(round(test_rmse, 6))
        test_results['Test R2'].append(round(test_r2, 6))
        test_results['Test Time (s)'].append(round(test_time / 1000, 6))  # Convert back to seconds

        print(f"Model: {model_name}\nBest Hyperparameters: {random_search.best_params_}\nTrain MSE: {round(train_mse, 6)}, Train RMSE: {round(train_rmse, 6)}, Train R2: {round(train_r2, 6)}, Train Time (s): {round(train_time / 1000, 6)}\nTest MSE: {round(test_mse, 6)}, Test RMSE: {round(test_rmse, 6)}, Test R2: {round(test_r2, 6)}, Test Time (s): {round(test_time / 1000, 6)}\n{'='*50}")

    return best_hyperparameters, train_results, test_results

# Calculate Spearman correlation coefficients
correlation_scores = []
for column in df.drop('Fee', axis=1).columns:
    correlation, _ = spearmanr(df[column], df['Fee'])
    correlation_scores.append((column, abs(correlation)))




# Sort features by absolute correlation coefficient
correlation_scores.sort(key=lambda x: x[1], reverse=True)

# Select the top k features
k = 24  # Number of features to select
selected_features = [feat for feat, _ in correlation_scores[:k]]

# Get the selected features from the DataFrame
X = df.drop('Fee', axis=1)[selected_features]
y = df['Fee']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Evaluating regression models
best_hyperparameters, train_results, test_results = evaluate_regression_models(X_train, X_test, y_train, y_test)

# Create a table for train results using Plotly
train_results_df = pd.DataFrame(train_results)
fig1 = go.Figure(data=[go.Table(header=dict(values=list(train_results_df.columns)),
                                cells=dict(values=[train_results_df[col] for col in train_results_df.columns]))])
fig1.show()

# Create a table for test results using Plotly
test_results_df = pd.DataFrame(test_results)
fig2 = go.Figure(data=[go.Table(header=dict(values=list(test_results_df.columns)),
                                cells=dict(values=[test_results_df[col] for col in test_results_df.columns]))])
fig2.show()

import time
import matplotlib.pyplot as plt


train_results_df = pd.DataFrame(train_results)
test_results_df = pd.DataFrame(test_results)

# Create bar plots for training time
plt.figure(figsize=(10, 6))
plt.bar(train_results_df['Model'], train_results_df['Train Time (s)'], color='skyblue')
plt.xlabel('Model')
plt.ylabel('Training Time (seconds)')
plt.title('Training Time for Different Models')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Create bar plots for testing time
plt.figure(figsize=(10, 6))
plt.bar(test_results_df['Model'], test_results_df['Test Time (s)'], color='lightgreen')
plt.xlabel('Model')
plt.ylabel('Testing Time (seconds)')
plt.title('Testing Time for Different Models')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

model = LinearRegression()
model.fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Plotting training set
plt.figure(figsize=(10, 5))
plt.scatter(y_train, y_train_pred, color='blue', label='Actual vs Predicted (Training)')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression - Training Set')
plt.legend()
plt.grid(True)
plt.show()

# Plotting test set
plt.figure(figsize=(10, 5))
plt.scatter(y_test, y_test_pred, color='red', label='Actual vs Predicted (Test)')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression - Test Set')
plt.legend()
plt.grid(True)
plt.show()

# def evaluate_regression_models(X_train, X_test, y_train, y_test):

#     np.random.seed(42)

#     models = {
#         'Linear Regression': LinearRegression(),
#         'Ridge Regression': Ridge(random_state=42),
#         'Lasso Regression': Lasso(random_state=42),
#         'Random Forest Regressor': RandomForestRegressor(random_state=42),
#         'AdaBoost Regressor': AdaBoostRegressor(random_state=42),
#         'XGBoost Regressor': xgb.XGBRegressor(random_state=42),
#         'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=42),
#         # 'LGBM Regressor': LGBMRegressor(random_state=42,verbose=-1)  # Remove LGBMRegressor
#     }

#     best_hyperparameters = {}
#     train_results = {'Model': [], 'Train MSE': [], 'Train RMSE': [], 'Train R2': [], 'Train Time (s)': []}
#     test_results = {'Model': [], 'Test MSE': [], 'Test RMSE': [], 'Test R2': [], 'Test Time (s)': []}

#     for model_name, model in models.items():
#         param_distributions = {}
#         if model_name in ['Ridge Regression', 'Lasso Regression']:
#             param_distributions = {'alpha': uniform(0, 100)}
#         elif model_name == 'Random Forest Regressor':
#             param_distributions = {'n_estimators': randint(100, 200), 'max_depth': [None, 10, 20], 'min_samples_split': randint(2, 10)}
#         elif model_name == 'AdaBoost Regressor':
#             param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1)}
#         elif model_name == 'XGBoost Regressor':
#             param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1), 'max_depth': randint(2, 7)}
#         elif model_name in ['Gradient Boosting Regressor', 'LGBM Regressor']:  # Modify for LGBMRegressor
#             param_distributions = {'n_estimators': randint(100, 200), 'learning_rate': uniform(0.01, 0.1), 'max_depth': randint(2, 7)}

#         start_train_time = time.time()
#         random_search = RandomizedSearchCV(model, param_distributions, n_iter=10, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1, random_state=42)
#         random_search.fit(X_train, y_train)
#         end_train_time = time.time()

#         best_hyperparameters[model_name] = random_search.best_params_

#         best_model = random_search.best_estimator_
#         best_model.fit(X_train, y_train)

#         # Print selected features
#         print(f"Selected features for {model_name}: {X_train.columns}")

#         start_pred_time_test = time.time()
#         y_train_pred = best_model.predict(X_train)
#         y_test_pred = best_model.predict(X_test)
#         end_pred_time_test = time.time()

#         train_mse, test_mse = mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred)
#         train_rmse, test_rmse = np.sqrt(train_mse), np.sqrt(test_mse)
#         train_r2, test_r2 = r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)

#         train_time = end_train_time - start_train_time
#         test_time = end_pred_time_test - start_pred_time_test

#         train_results['Model'].append(model_name)
#         train_results['Train MSE'].append(round(train_mse, 6))
#         train_results['Train RMSE'].append(round(train_rmse, 6))
#         train_results['Train R2'].append(round(train_r2, 6))
#         train_results['Train Time (s)'].append(round(train_time, 6))

#         test_results['Model'].append(model_name)
#         test_results['Test MSE'].append(round(test_mse, 6))
#         test_results['Test RMSE'].append(round(test_rmse, 6))
#         test_results['Test R2'].append(round(test_r2, 6))
#         test_results['Test Time (s)'].append(round(test_time, 6))

#         print(f"Model: {model_name}\nBest Hyperparameters: {random_search.best_params_}\nTrain MSE: {round(train_mse, 6)}, Train RMSE: {round(train_rmse, 6)}, Train R2: {round(train_r2, 6)}, Train Time (s): {round(train_time, 6)}\nTest MSE: {round(test_mse, 6)}, Test RMSE: {round(test_rmse, 6)}, Test R2: {round(test_r2, 6)}, Test Time (s): {round(test_time, 6)}\n{'='*50}")

#     return best_hyperparameters, train_results, test_results